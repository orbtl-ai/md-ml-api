import tensorflow as tf

def load_model(path_to_pb):
    '''
    A simple function that loads a Tensorflow model from a saved_model.pb file.
    '''
    model = tf.saved_model.load(path_to_pb)
    return model

def denormalize_coordinates(list_of_bboxes, im_width=512, im_height=512):
    '''
    A simple funtion that takes normalized bounding box image coordinates (0-1.0) and converts to absolute image pixel coordinates.
    '''
    # TODO HOW DO I GET THE HEIGHT/WIDTH W/O PROMPTING USER OR HARDCODING??
    px_bboxes = []
    for bbox in list_of_bboxes:
        xmin = bbox[0]
        ymin = bbox[1]
        ymax = bbox[3]
        xmax = bbox[2]

        left   = int(xmin * im_width)
        right  = int(xmax * im_width)
        top    = int(ymin * im_height)
        bottom = int(ymax * im_height)

        px_coords = [left, top, right, bottom]
        px_bboxes.append(px_coords)

        #print(px_bboxes)
    return px_bboxes

def batch_inference(dict_of_tensors, model, CONFIDENCE_THRESHOLD) -> dict:
    '''
    The main inference function of the ML-of-MD backend API. This function ingests a Python dictionary that contains the image chip names as keys, and
    each image chip converted to a 3-D numpy array as values. Each image chip array is padded to TF's desired dimension and inference is performed. The
    resulting model detections are reformatted back into a Python dictionary that contains detections.
    INPUTS: 
      -  dict_of_tensors: a Python dictionary containing each chipped image array and the associated image chip name.
           -  KEYS: image chip names
           -  VALUES: 3-D numpy arrays of the image chip
      -  model: A pre-trained Tensorflow model initialized from a saved_model.pb folder (this is expected to have been generated by the backend API's 
                load_model() function).
    TODO: TF's model() routine naturally takes input arrays of dim [n_images, HEIGHT, WIDTH, BANDS] with expected shape (n, 512, 512, 3). For some reason my
            exported model only allows input arrays of shape (1, 512, 512, 3). For this reason I need to loop over each image chip array and pad the first dimension.
            I should be able to just concatenate all the image chip arrays along the first axis and feed into model() as a single batch.
    '''
    
    results = {}
    for k, v in dict_of_tensors.items():
        individual_results = {}
        
        padded_tensor = v[tf.newaxis, ...]

        detections = model(padded_tensor) # Run model inference

        detection_scores = detections['detection_scores'][0].numpy() 
        individual_results['scores'] = detection_scores[detection_scores >= CONFIDENCE_THRESHOLD].tolist()

        detection_bboxes = detections['detection_boxes'][0].numpy()[detection_scores >= CONFIDENCE_THRESHOLD].tolist()
        individual_results['bboxes'] = denormalize_coordinates(detection_bboxes)
        
        individual_results['classes'] = detections['detection_classes'][0].numpy()[detection_scores >= CONFIDENCE_THRESHOLD].astype('uint8').tolist()
       
        results[k] = individual_results
        
    return results